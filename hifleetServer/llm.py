from typing import Annotated
from typing_extensions import TypedDict
from dataclasses import dataclass, field
from typing import List
from langchain.tools import tool

from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage
from config import huoshan_model_deepseekv3_1, huoshan_base_url, huoshan_api_key
import os
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition
#æ·»åŠ è®°å¿†
from langgraph.checkpoint.memory import MemorySaver

# 1. åˆå§‹åŒ–ç«å±±æ–¹èˆŸæ¨¡å‹ï¼ˆå…¼å®¹ OpenAI APIï¼‰
# --------------------------
huoshanModel = ChatOpenAI(
    model=huoshan_model_deepseekv3_1,
    base_url=huoshan_base_url,
    api_key=huoshan_api_key
)

# å®šä¹‰å·¥å…· Tool
@tool
def multiply(a: int,b: int) -> int:
    """è®¡ç®—ä¸¤ä¸ªæ•´æ•°ç›¸ä¹˜çš„ç»“æœã€‚åªç”¨äºè®¡ç®—ä¹˜æ³•ã€‚"""
    print(f"å·¥å…·è§¦å‘ï¼Œæ­£åœ¨è®¡ç®—{a}*{b}...")
    return a*b

@tool
# å°†å·¥å…·æ”¾å…¥ä¸€ä¸ªåˆ—è¡¨
tools = [multiply]
#å…³é”®æ­¥éª¤ï¼šå‘Šè¯‰å¤§æ¨¡å‹å®ƒæœ‰å“ªäº›å·¥å…·å¯ç”¨ (bind_tools)
# è¿™ç›¸å½“äºç»™äº†å¤§æ¨¡å‹ä¸€æœ¬â€œå·¥å…·è¯´æ˜ä¹¦â€
llm_with_tools = huoshanModel.bind_tools(tools)


# 2. å®šä¹‰ LangGraph çŠ¶æ€Stateå’ŒèŠ‚ç‚¹Node
class State(TypedDict):
    # messages æ˜¯ä¸€ä¸ªåˆ—è¡¨ã€‚
    # Annotated[list, add_messages] çš„æ„æ€æ˜¯ï¼š
    # å½“æœ‰æ–°æ¶ˆæ¯è¿”å›æ—¶ï¼Œä¸è¦è¦†ç›–åŸæ¥çš„ï¼Œè€Œæ˜¯â€œè¿½åŠ â€ï¼ˆAppendï¼‰åˆ°åˆ—è¡¨åé¢ã€‚
    # è¿™å°±æ˜¯ AI æ‹¥æœ‰â€œè®°å¿†â€çš„åŸºç¡€ã€‚
    messages:Annotated[list, add_messages]

# 3.å®šä¹‰Node èŠ‚ç‚¹
def chat_node(state: State):
    #1.è·å– State çš„ messages
    messages = state["messages"]
    #2.è°ƒç”¨ huoshanModel
    response = huoshanModel.invoke(messages)
    #3.è¿”å›response 
    #æ³¨ï¼šè¿™é‡Œè¿”å›çš„å­—å…¸ä¼šè‡ªåŠ¨åˆå¹¶å…¥Stateä¸­çš„messages ,åŸºäº add_messagesé€»è¾‘ï¼šå½“æœ‰æ–°æ¶ˆæ¯è¿”å›æ—¶ï¼Œä¸è¦è¦†ç›–åŸæ¥çš„ï¼Œè€Œæ˜¯â€œè¿½åŠ â€ï¼ˆAppendï¼‰åˆ°åˆ—è¡¨åé¢
    return {"messages": [response]}

def agent_node(state: State):
    print("AIæ€è€ƒä¸­")
    #æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è°ƒç”¨çš„æ˜¯ç»‘å®šäº†å·¥å…·çš„ llm_with_tools
    return {"messages":[llm_with_tools.invoke(state["messages"])]}

#ToolNode æ˜¯ LangGraphæä¾›çš„ä¸€ä¸ªé¢„åˆ¶èŠ‚ç‚¹ï¼šä¸“é—¨æ£€æµ‹AIæ˜¯å¦å‘èµ·äº†å·¥å…·è°ƒç”¨è¯·æ±‚ -> è¿è¡Œå·¥å…· -> è¿”å›ç»“æœ
tool_node = ToolNode(tools)



#4.ç»˜åˆ¶Graph å›¾
#4.1 åˆ›å»ºä¸€ä¸ªGraphå®ä¾‹ workflow
workflow = StateGraph(State) 
#4.2 æ·»åŠ ä¸¤ä¸ªèŠ‚ç‚¹
# workflow.add_node("chatbot", chat_node)
workflow.add_node("agent", agent_node)
workflow.add_node("tools", tool_node)
#4.3 å®šä¹‰è¾¹
# Startæ˜¯å›¾çš„å…¥å£ï¼šå¯åŠ¨åç›´æ¥è¿›å…¥nodeï¼šâ€œchatbotâ€
# workflow.add_edge(START,"chatbot")
workflow.add_edge(START,"agent")

#5.å…³é”®ï¼è®¾ç½®conditional edge æ¡ä»¶è¾¹
# 1. ä» "agent" èŠ‚ç‚¹å‡ºæ¥åï¼Œä¸è¦ç›´æ¥èµ°ï¼Œå…ˆåœä¸‹æ¥çœ‹ä¸€çœ¼ (tools_condition)ã€‚
# 2. tools_condition æ˜¯ LangGraph è‡ªå¸¦çš„é€»è¾‘ï¼š
#    - å¦‚æœ AI è¯´â€œæˆ‘è¦è°ƒç”¨å·¥å…·â€ï¼Œå°±å» "tools" èŠ‚ç‚¹ã€‚
#    - å¦‚æœ AI è¯´â€œä½ å¥½/è®²ä¸ªç¬‘è¯â€ï¼ˆä¸éœ€è¦å·¥å…·ï¼‰ï¼Œå°±å» ENDã€‚
workflow.add_conditional_edges(
    "agent",         #å‡ºå‘ç‚¹
    tools_condition, #è·¯ç”±é€»è¾‘
)

#6å®šä¹‰é—­ç¯
# å·¥å…·å¹²å®Œæ´» ("tools")ï¼Œå¿…é¡»å›åˆ° "agent" è®©å¤§è„‘å†æ•´ç†ä¸€ä¸‹ç»“æœ
workflow.add_edge("tools", "agent")



#7.ç¼–è¯‘ä¸è¿è¡Œ,å¹¶é…ç½®è®°å¿†
#compile å°† ç”»çº¸ ç¼–è¯‘æˆ ç¨‹åº
# app = workflow.compile()
#7.1 åˆå§‹åŒ–å†…å­˜ç®¡ç†å™¨ memory
memory = MemorySaver()
#7.2 ç¼–è¯‘æ—¶ï¼Œæ ‡æ³¨å›¾ç”¨ è¯¥ç®¡ç†å™¨ä¿å­˜çŠ¶æ€
app  = workflow.compile(checkpointer=memory)

#è®¾ç½®è¿›ç¨‹id
thread_config = {"configurable":{"thread_id":"user_1_session"}}


## è°ƒè¯•ï¼Œæµ‹è¯•æ—¥å¿—
print("\n--- ğŸ•µï¸â€â™‚ï¸ è¿›å…¥ä¾¦æ¢æ¨¡å¼ï¼šé€å¸§æŸ¥çœ‹è¿è¡Œè¿‡ç¨‹ ---")

inputs = {"messages": [("user", "è®¡ç®— 5 ä¹˜ä»¥ 5ï¼Œç„¶åå†æŠŠç»“æœä¹˜ä»¥ 10")]}
config = {"configurable": {"thread_id": "debug_session_1"}}

# å…³é”®ç‚¹ï¼šä½¿ç”¨ stream è€Œä¸æ˜¯ invoke
# stream_mode="values" çš„æ„æ€æ˜¯ï¼šæ¯ç»è¿‡ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå°±æŠŠå½“å‰çš„æ•´ä¸ª State æ‰“å°å‡ºæ¥
for event in app.stream(inputs, config=config, stream_mode="values"):
    
    # event å…¶å®å°±æ˜¯å½“å‰çš„ State
    messages = event["messages"]
    last_message = messages[-1]
    
    # æ‰“å°å½“å‰æœ€æ–°çš„é‚£æ¡æ¶ˆæ¯æ˜¯è°å‘å‡ºçš„ï¼Œå†…å®¹æ˜¯ä»€ä¹ˆ
    # type å¯èƒ½æ˜¯ 'human', 'ai', 'tool'
    print(f"ğŸ“ [èŠ‚ç‚¹ç»“æŸ] æœ€æ–°æ¶ˆæ¯ç±»å‹: {last_message.type}")
    print(f"   å†…å®¹: {last_message.content}")
    
    # å¦‚æœæ˜¯å·¥å…·è°ƒç”¨ï¼Œæ‰“å°ä¸€ä¸‹ç»†èŠ‚
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        print(f"   ğŸ› ï¸  AI å†³å®šè°ƒç”¨å·¥å…·: {last_message.tool_calls}")
    
    print("-" * 30)
    
# print("\n--- ğŸ—ºï¸  æµç¨‹åœ°å›¾ ---")
# # try:
# print(app.get_graph().draw_ascii())
# except Exception as e:
#     print("æ‰“å°å›¾è¡¨éœ€è¦å®‰è£… extra ä¾èµ–ï¼Œå¦‚æœæŠ¥é”™å¯ä»¥è·³è¿‡ã€‚")

# ## æµ‹è¯•è®°å¿†
# user_input1 = "æˆ‘çš„å¹¸è¿æ•°å­—æ˜¯10"
# response_1 = app.invoke(
#     {"messages":[HumanMessage(content=user_input1)]},
#     config = thread_config
# )
# print(f"user:{user_input1}\n AI:{response_1['messages'][-1].content}")
# user_input2 = "æˆ‘çš„å¹¸è¿æ•°å­—ä¹˜äº5æ˜¯å¤šå°‘"
# response_2 = app.invoke(
#     {"messages":[HumanMessage(content=user_input2)]},
#     config = thread_config
# )
# print(f"user:{user_input2}\n AI:{response_2['messages'][-1].content}")


## æµ‹è¯• tool è°ƒç”¨
# #è¾“å…¥ï¼š
# print('ç®€å•å¯¹è¯')
# answer = app.invoke({"messages":[("user","ä½ æ˜¯è°")]})
# print(f"user ask:{query}\n,llm answer{answer['messages'][-1].content}")
# print("Aråº”è¯¥ç›´æ¥å›å¤ï¼Œä¸è°ƒç”¨å·¥å…·")

# print('æµ‹è¯•å·¥å…·è°ƒç”¨')
# query = "è®¡ç®— 12345 ä¹˜ä»¥ 6789 ç­‰äºå¤šå°‘ï¼Ÿ"
# print(f"ç”¨æˆ·: {query}")

# user_input = "hello,please introduce langgraph"
# system_inout = ""
# #invoke è§¦å‘è¿è¡Œï¼Œä¼ å…¥åˆå§‹çŠ¶æ€ Stateï¼Œè¿”å›æœ€ç»ˆçŠ¶æ€
# # final_state = app.invoke({"messages": [("user",user_input)]})
# #OpenAI APIï¼‰è¦æ±‚ messages æ˜¯ BaseMessage å®ä¾‹åˆ—è¡¨ï¼ˆå¦‚ HumanMessage/SystemMessage
# final_state = app.invoke({
#     "messages":[
#         HumanMessage(content=query)
#         # SystemMessage(content=system_input)
#     ]
# })

# print(f"user ask:{query}\n,llm answer{final_state['messages'][-1].content}")
